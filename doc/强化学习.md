# 介绍
传统有监督学习需要提前准备好数据和标签，而强化学习会对选择的决策进行打分，然后之后会尽量选取高分的决策，等于就是将决策和分数对应起来，等同于有监督学习中的数据和标签
# 分类
## 环境建模并从中学习
- Model based RL：如果没有环境建模，系统只能作出决策然后等待真实系统反馈然后进行打分，这里的环境建模就像是建立了一个仿真系统，系统可以更高效地获得反馈。建立好模型之后，采用的方式和上面完全相同
## 基于概率（policy based）
**可以输出一系列决策的概率**
- Policy gradient
## 基于价值
可以输出一系列决策的价值，选取最高的。但如果是一个连续的动作，就不大合适用基于价值的方法
- Q learning：表格学习
- Sarsa
- Deep Q Network：神经网络学习
## 结合价值和策略
- actor-critic
## 回合更新
- 基础版Policy gradient
- Monte-carlo learning
## 单步更新
- Q learning：表格学习
- Sarsa
- 升级版Policy gradient
## 在线学习（on policy）
自己学习
- Sarsa
## 离线学习（off policy）
看别人学，用别人的经历学习
- Q learning：表格学习
- Deep Q Network：神经网络学习

# 机器学习
![](Markdown_md_files/27c813b0-cb96-11ed-a071-c10aa0561344.jpeg?v=1&type=image)
# SVM的局限
- 人工设计特征不充分
- 无法处理大数据
- 无法处理无结构的数据
# 深度学习实践
- [笔记1](http://biranda.top/)
- [笔记2](https://blog.csdn.net/bit452/category_10569531.html)
## 梯度下降
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/b91e2d20-cb9d-11ed-a071-c10aa0561344_20230326141745.jpeg?v=1&type=image&token=V1:7P_JaZqhlqJy0WImJEttJ54hLJPPJxg79Bn0NTy6pHc)
$\alpha: learning\_rate$
需要克服的问题
	- 局部最优
	- 鞍点
## 随机梯度下降
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/8f3f6ad0-cb9f-11ed-a071-c10aa0561344_20230326142931.jpeg?v=1&type=image&token=V1:mUAnfCKN67h0T8EsBqV70M71q5ilxawoLGea0Y1DKHU)
主要区别是传统梯度下降是采用所有样本数据本轮的loss均值作为cost，而随机梯度下降则是选取一个样本loss作为cost，这种做法可以帮助跨过鞍点。但是会导致一个问题，传统梯度下降算法等于是一轮迭代一次，所以不同样本的训练可以并行，但是随机梯度下降是每个样本迭代一次，无法并行计算。所以深度学习实践中，一般采用折中的方式，把数据分成若干个batch，每个batch里有几个样本，每次的cost就是这个batch里的样本的loss均值，这几个样本训练可以并行，这样既增加了效率，也可以获得更好的学习效果
## 反向传播（back propagation）
对于简单模型，可以推导得到对某个参数的梯度来进行梯度下降算法，但是一旦模型有一定规模，就不容易得到梯度的表达式了，所以可以采用back propagation方法，他可以把整个模型分层，通过链式求导来自动求导
矩阵求导（[matrix cookbook](https://max.book118.com/html/2017/0424/102092083.shtm)）
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/ecfa23b0-cba2-11ed-a071-c10aa0561344_20230326145336.jpeg?v=1&type=image&token=V1:I3w5jEPqj-RnS-pDVe4IM-fBHKUoFiqH1oNy54R6Bqc)
如图可以得到Loss对于x和w的偏导（梯度），从而更新x和w
## 非线性层
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/3193e570-cba2-11ed-a071-c10aa0561344_20230326144822.jpeg?v=1&type=image&token=V1:e7IgUAjxag4ZgGD0DLb_E0pYpItUUhlr78GI_00TvCk)
从这个推导可以看到无论多少层的神经网络最终都可以转化成一层，那设计就没有意义了，所以需要添加一个非线性层，得到结构如下
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/58e904c0-cba2-11ed-a071-c10aa0561344_20230326144928.jpeg?v=1&type=image&token=V1:fngZb2YRqiI71600-8Qw0rRr2k3GtdBwma9R5_378eQ)
## logistic regression（二分类）
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/d1e7ad10-cbaf-11ed-a071-c10aa0561344_20230326162555.jpeg?v=1&type=image&token=V1:Xu3p3nFyQhDTkfSm6fiGYuuvpb0t3Jmo48zJztsKJ_E)
他虽然是一个回归方法（输出是连续的值），但是可以映射到[0,1]之间，来获得概率，用来实现二分类。
映射方法有很多，比如sigmoid之类的函数都可以，统称为logsitic function
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/8e86e7e0-cbad-11ed-a071-c10aa0561344_20230326160943.jpeg?v=1&type=image&token=V1:2EGVUkQ61GQXi6Ref9_O8zSekg_4h5NdztKliNnSB_c)
## 二分类问题的loss
回归问题是求一个值，可以直接通过数据计算来求loss，但是分类问题其实得到的是每个类的概率（二分类只有一个类，概率是0和1），是一个分布，这时的loss设计不大一样，用交叉熵（cross entropy）用来衡量两个分布的区别，这里的n指的是batch的大小
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/98d4c670-cbaf-11ed-a071-c10aa0561344_20230326162419.jpeg?v=1&type=image&token=V1:lXEbWdm3FbV7m_epUFYbllTipznquoEEw1BjEPTWP5k)
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/07069d30-cbb0-11ed-a071-c10aa0561344_20230326162724.jpeg?v=1&type=image&token=V1:JhpV1bcaQcoNsdDrWt-SWK9TSiiU_vNpnJdiYqiTYjY)
## 多维输入的logistics regression
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/3d202480-cbb1-11ed-a071-c10aa0561344_20230326163604.jpeg?v=1&type=image&token=V1:MU1PvqbundZv9SzhRZgEVElZACfpu8p92SdWe4Qfryg)
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/c0460e60-cbb1-11ed-a071-c10aa0561344_20230326163944.jpeg?v=1&type=image&token=V1:Od9XFRhMwewOqqrZyx8483ajkWXOMOioV8G3h8N8tr8)
其实多维输入和输出之间并不一定是线性的关系，但是我们可以用过设计多个线性层，来拟合，线性层之间需要添加非线性层，否则无论多少个线性层最终都可以等效成一个线性层

## 数据加载
- epoch：所有样本训练一次算1epoch
- batch：1个epoch里数据被分成N个batch
- iteration：分成N个batch就会迭代N次

## 多分类问题
- 输出
要求输出的是每个类的概率（大于0），且总和等于1，可以通过softmax层来实现
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/a5e6df00-cbba-11ed-a071-c10aa0561344_20230326174325.jpeg?v=1&type=image&token=V1:v7A7iw7NKaUjdVSikcsPwdUQoMakQdrH8zfHLRgb8PY)
- CrossEntropyLoss <==> LogSoftmax + NLLLoss
也就是说使用CrossEntropyLoss最后一层(线性层)是不需要做其他变化的；使用NLLLoss之前，需要对最后一层(线性层)先进行SoftMax处理，再进行log操作。
## 卷积神经网络
- 数据个数
	- 图像：Channel x Width x Height
- 输入输出通道
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/0f0a4a60-cbc2-11ed-a071-c10aa0561344_20230326183625.jpeg?v=1&type=image&token=V1:DBsWGdtZ-9RezlJy0bQRdWBvg41jOkG9-aMTgnorL3s)
	- 卷积核的通道数和输入数据的通道数相同
	- 卷积核的个数和输出数据的通道数相同
## 高级卷积神经网络
- GoogleNet
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/22ca1150-cbc4-11ed-a071-c10aa0561344_20230326185117.jpeg?v=1&type=image&token=V1:Ki2xlN0NDduxOyJ2GZqHfEE_acBBiqViuvfd2BYehCE)
- inception
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/313f6460-cbc4-11ed-a071-c10aa0561344_20230326185142.jpeg?v=1&type=image&token=V1:0GLkOhN7HJ6px6Euvc8dL8yk26R1x2QutOmCxumyksA)
这里是设置了多个不同尺寸的卷积核，因为不知道哪种最好，这样在训练过程中会自动找到效果最好的那一种卷积核
- 1x1的卷积核
等于就是原始数据乘上卷积核的值，然后多个通道加起来（不同通道卷积核的值也不同，可以理解为不同通道的权重），目的就是融合同个像素位置多个通道的信息，可以改变通道数量，降低计算量
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/49f91590-cbc5-11ed-a071-c10aa0561344_20230326185933.jpeg?v=1&type=image&token=V1:BBNECK0MtyfdoCsoPgNnsggO8l0sQ7r5UWAVOlgE9-A)
- 梯度消失（ResNet）
因为梯度是通过链式法则求导相乘得到的，如果每个梯度都很小，那么他们的乘积会更小，如果趋近于0，那么就迭代不动了。可以通过residual net（ResNet）来解决
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/c04e97e0-cbc7-11ed-a071-c10aa0561344_20230326191710.jpeg?v=1&type=image&token=V1:RoftBZEmlN9LN2hCCATxXkzoHpS9kmmJpuI5ZuWx3RA)
**这里需要注意F(x)和x的维度要相等**
如果发生梯度消失，那么说明
$$\frac{\partial L}{\partial x}=\frac{\partial L}{\partial F}\frac{\partial F}{\partial x}\rightarrow0$$
加上residual net之后
$$\frac{\partial L}{\partial x}=\frac{\partial L}{\partial H}\frac{\partial H}{\partial x}=\frac{\partial L}{\partial H}(\frac{\partial F}{\partial x}+1)=\frac{\partial L}{\partial F}(\frac{\partial F}{\partial x}+1)$$
这里需要注意
$$\frac{\partial L}{\partial F}=\frac{\partial L}{\partial H}$$
因为这两者都表示这个block输出之后的所有环节的梯度
所以这种设置可以避免出现梯度消失的问题
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/ab07b030-cbcf-11ed-a071-c10aa0561344_20230326201351.jpeg?v=1&type=image&token=V1:zdTehDCHRD1bS0hAl6MHIwMbmfyzMZh9fMowHu7jfsc)

## 循环神经网络
专门用于处于序列数据
### RNN Cell（只有一个输出）
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/cc838b50-cbdc-11ed-a071-c10aa0561344_20230326214750.jpeg?v=1&type=image&token=V1:0PZ2-u7_g8LSADkIRyKjM4vlRo3LbjN-WJIxLSO6hPI)
这两个模型等价，x是输入序列，ht是最后的输出
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/0ef79d00-cbd3-11ed-a071-c10aa0561344_20230326203807.jpeg?v=1&type=image&token=V1:rYKErcH1gK45ljtVV56gLh_G-8MJ3e4j1_XBNedlFBY)
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/7e37af70-cbd3-11ed-a071-c10aa0561344_20230326204113.jpeg?v=1&type=image&token=V1:7rmivU62kHEfb68MBamMTHELNcKusb7wO-WNYxyM7bg)
需要注意
- 每个RNN cell都是同一个，实现权重共享
- 每个RNN cell输出的h会作为下一个RNN cell的输入
- 初始的h0是作为前置信息，因为可能需要将RNN接在其他网络后面，若没有可获知的前置信息，可以将h0设置为0即可
- input_size：原数据序列单元里数据的维度
- hidden_size：隐藏层数据的维度
- dataset: (seq_len, batch_size, input_size)
## RNN（多个输出）
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/ffb79b60-cbdc-11ed-a071-c10aa0561344_20230326214916.jpeg?v=1&type=image&token=V1:kVHaw9vK_ve8zi1YZBKtjrciCe3A3ASQFmk-0ZcA8BE)
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/aa396a10-cbd7-11ed-a071-c10aa0561344_20230326211105.jpeg?v=1&type=image&token=V1:5JKF3xaCOVnLlRo9rJz9pBeIyS4536t4M1GMTAOvdyw)
用RNN cell只会输出ht，但是用RNN可以将所有的h都输出，两者只有numlayer的区别，相同颜色的RNN Cell是同一个
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/55f71320-cbdd-11ed-a071-c10aa0561344_20230326215141.jpeg?v=1&type=image&token=V1:TNJvxFBML13M7y2pRb4T0kBpxQzZk4njB6O85W0BgVk)

## 例子1
目标：假定现在有一个序列到序列（seq→seq）的任务，比如将“hello”转换为“ohlol”。
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/155f2a90-cbde-11ed-a071-c10aa0561344_20230326215702.jpeg?v=1&type=image&token=V1:tdLoMeIAB51pWP2m3eBka-Lnza1ntafxERUlxNemaIg)
- 输入
原输入“hello”并不是一个向量，需要将其转变为一组数字向量。
对**输入序列**和的每一个字符（单词）构造字典（词典），此时每一个字符都会有一个唯一的数字与其一一对应。即将字符向量转换为数字向量。之后利用独热编码（One-Hot）的思想，即可将每个数字转换为一个向量，比如第一个输入h，等于输入0100，所以input_size=4
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/e5e4a010-cbdd-11ed-a071-c10aa0561344_20230326215542.jpeg?v=1&type=image&token=V1:EzZfYoKYpJE5-ijEvh8clDC83s7oK9ZDyQb-03U5TU0)
- 输出
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/339b3a70-cbdf-11ed-a071-c10aa0561344_20230326220502.jpeg?v=1&type=image&token=V1:eOOwX_x5FhfKQqAEYEfXrayog4588fzfUpFQKZS_4DQ)
两个e其中一个应该改成h
输出可以服用输入字符的字典，所以对于每一个输入，都有一个概率分布来表示当前输入是每种字符的概率，所以输出维度也为4。也可以重新构造字典，因为只用到了h，o，l三个字符，这样输出维度可以是3
## 嵌入层（embedding）
one-hot来编码有几个缺点
- 维度过高
- 输入数据是稀疏矩阵
- 编码过程写成hardcode
采用嵌入层的优点
- 数据维度低
- 稠密
- 来自学习
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/ffbd0ae0-cbe2-11ed-a071-c10aa0561344_20230326223214.jpeg?v=1&type=image&token=V1:TAW5oJnLRmZyPrIHGIqunYiWFTMgMkLeMV5IhLI2FoM)
其实embedding只是取代了one-hot，但构造字典还是要做的，所以原字符还是需要转化成数字，几种数字，input_size就是几，embedding的输出可以理解为数字是几就输出embedding矩阵的第几行（这么说one-hot也可以理解成是一种embedding矩阵？）
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/7473d8b0-cbe2-11ed-a071-c10aa0561344_20230326222820.jpeg?v=1&type=image&token=V1:DNr1C08Dz-753EUHzyH_VcV0NPdihQGbJeNG13RAyYU)
## 双向RNN
因为对于序列数据，当前的输出不止和过去的数据相关，也和未来的数据相关
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/a5854f80-cbe5-11ed-a071-c10aa0561344_20230326225111.jpeg?v=1&type=image&token=V1:79Uz6agDW0tZjoyfLa2SoWXfciJcR3PedcdBl9neaMs)
## 例子2
假定现在有一份关于名字的数据集
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/28a5f420-cbe4-11ed-a071-c10aa0561344_20230326224032.jpeg?v=1&type=image&token=V1:srh9c5dj4_X5bXeKsEJ808Jbi4h2u4f4cUCu4taBHxM)
核心问题在于，判断数据集中的每个名字所属的国家，共有18个国家类别。显然，每个国家或地区的人取名字都有其自己独特的语言习惯，因此可以利用RNN分析其名字（字符串）的潜在特点来进行分类。
- input_size：字母表的size
- output_size：国家的个数

![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/f0b35520-cbe4-11ed-a071-c10aa0561344_20230326224607.jpeg?v=1&type=image&token=V1:Hc2UFs7z7gDBqcSHS_Ut01nwQSmG_7aiufUhjYWJrjg)
因为不像例子1那样需要对每个字符串进行变换，所以只需要最终输出即可
- 输入处理
输入是每个字符转换数字之后要填充再转置
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/9e2fa580-cbe7-11ed-a071-c10aa0561344_20230326230517.jpeg?v=1&type=image&token=V1:DLoLo4fC-DQMkcU6f5VUdMeQEQZs88qGjpQif56LbiA)
然后经过embedding
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/158009f0-cbe7-11ed-a071-c10aa0561344_20230326230128.jpeg?v=1&type=image&token=V1:_loM2uXWIpgcIwr0iMzq6doS2FMhfOnIP_E9Xwo06hg)
所以现在的列数就是batch_size，也就是输入字符的个数。因为每一个数字都会通过embedding编码，转化成一个向量，但是这边是hidden_size（之前是embedding_size）
为了简化计算，把填充的部分去掉，需要先按照长度排序
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/dc6e3f50-cbe7-11ed-a071-c10aa0561344_20230326230702.jpeg?v=1&type=image&token=V1:7di0W263WiLCxh101O8djJgCCT1BGY6io8yTshyK044)
然后去掉之前填充的部分，依次排列成一列
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/eb1fd810-cbe7-11ed-a071-c10aa0561344_20230326230726.jpeg?v=1&type=image&token=V1:LMogfaZBAC9HJ4BV5ex4dgHDU5BkzCCG_2oXx0Ate8I)

## RNN在车辆模型拟合上的作用
![](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5_md_files/aa396a10-cbd7-11ed-a071-c10aa0561344_20230326211105.jpeg?v=1&type=image&token=V1:5JKF3xaCOVnLlRo9rJz9pBeIyS4536t4M1GMTAOvdyw)
xi表示每个时刻的车辆状态，hi是目标量，

[toc]

# 基础知识
## 待读
- [2021->2022必看的十篇「深度学习领域综述」论文](https://zhuanlan.zhihu.com/p/452437749)
- [AI 数据集最常见的6大问题（附解决方案）](https://easyai.tech/blog/ai-dataset-6-problem-solution/)
- [周志华教授：关于深度学习的一点思考](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2OTA0NzE2NA%3D%3D%26mid%3D2247574935%26idx%3D2%26sn%3D7b17702653869252c0831f4a15e5d284%26chksm%3Dfc874484cbf0cd9210f26885cbe445e370327b4f8b7ed43e074bbe76ecc1441c25e25c01d79f%26scene%3D21%23wechat_redirect)
- [复旦大学邱锡鹏教授等「Transformers全面综述」论文](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2OTA0NzE2NA%3D%3D%26mid%3D2247559049%26idx%3D1%26sn%3D1038963e38a0738eddf22956e459eaa8%26chksm%3Dfc87069acbf08f8ce782b49038559a17bf70960e3ff69deb66db595023edc6680b419508f7b8%26scene%3D21%23wechat_redirect)
- [《机器人通过强化学习，可以实现像人一样的平衡控制》](https://baijiahao.baidu.com/s?id=1612651771159021082&wfr=spider&for=pc)
- [《深度学习与强化学习相结合，谷歌训练机械臂的长期推理能力》](https://www.leiphone.com/news/201807/yt7WTAhrQ0zXnnA5.html)
- [《伯克利强化学习新研究：机器人只用几分钟随机数据就能学会轨迹跟踪》](https://www.jiqizhixin.com/articles/2017-12-06-4)
## [机器学习](https://easyai.tech/ai-definition/machine-learning/#ai-ml-dl)
### 分类
- 有监督学习
使用有标签的数据来学习一个预测函数，目标是使预测的输出和真实的标签尽量一致，反馈是预测的误差
- 无监督学习
使用无标签的数据来学习数据的内在结构或分布，目标是使数据的表示更加简洁或有意义，反馈是数据的重构或概率
- 强化学习
使用无标签的数据来学习一个行为策略，目标是使行为的长期收益最大化，反馈是行为的奖励或惩罚
### 常用机器学习算法
#### 监督学习
- [线性回归](https://easyai.tech/ai-definition/linear-regression/)
- [逻辑回归](https://easyai.tech/ai-definition/logistic-regression/)
- [线性判别分析](https://easyai.tech/ai-definition/linear-discriminant-analysis/)
- [决策树](https://easyai.tech/ai-definition/decision-tree/)
- [朴素贝叶斯](https://easyai.tech/ai-definition/naive-bayes-classifier/)
- [K邻近](https://easyai.tech/ai-definition/k-nearest-neighbors/)
- [学习向量量化](https://easyai.tech/ai-definition/learning-vector-quantization/)
- [支持向量机](https://easyai.tech/ai-definition/svm/)
- [随机森林](https://easyai.tech/ai-definition/random-forest/)
- [AdaBoost](https://easyai.tech/ai-definition/adaboost/)
#### 无监督学习
- 高斯混合模型
- [限制波尔兹曼机](https://easyai.tech/ai-definition/restricted-boltzmann-machine/)
- [K-means 聚类](https://easyai.tech/ai-definition/k-means-clustering/)
- 最大期望算法
## [深度学习](https://easyai.tech/ai-definition/deep-learning/#guanxi)
深度学习其实属于一种工具，只要是通过搭建深度神经网络的学习方式都可以叫做深度学习，可以通过深度学习来进行上述的三种学习，常见的学习方式大多属于有监督的深度学习，也可以和强化学习结合来进行深度强化学习
![输入图片描述](https://easyai.tech/wp-content/uploads/2018/12/ai-ml-dl.png.webp)
下面介绍一些基础的深度学习网络，这些其实属于框架和类别，每个类别下还有很多著名的模型
### 有监督深度学习算法


#### [卷积神经网络（CNN）](https://easyai.tech/ai-definition/cnn/)
1. 特点
	- 主要是有监督（也可以无监督）
	- 能够将大数据量的图片有效的降维成小数据量(并不影响结果)
	- 保留图片的特征，类似人类的视觉原理
2. 原理
	- 卷积层 – 主要作用是保留图片的特征
	- 池化层 – 主要作用是把数据降维，可以有效的避免过拟合
	- 全连接层 – 根据不同任务输出我们想要的结果
3. 应用
	- 图片分类、检索
	- 目标定位检测
	- 目标分割
	- 人脸识别
	- 骨骼识别
4. 主流模型
	- AlexNet
	- VGG
		- SSD = VGG + region proposals
		- Faster-RCNN = VGG + ResNet
	- GoogleNet
		- Yolo = GoogleNet + bounding boxes
	- ResNet
#### [循环神经网络（RNN）](https://easyai.tech/ai-definition/rnn/)
1. 特点
主要是有监督（也可以无监督），能有效的处理序列数据的算法。比如：文章内容、语音音频、股票价格走势。之所以他能处理序列数据，是因为在序列中前面的输入也会影响到后面的输出，相当于有了“记忆功能”。但是 RNN 存在严重的短期记忆问题，长期的数据影响很小（哪怕他是重要的信息）。
2. 应用
	- 文本生成
	- 语音识别
	- 机器翻译
	- 生成图像描述
	- 视频标记
3. 主流模型
针对短期记忆问题，出现了一些变种，这些变种算法特点是长期信息可以有效的保留，并且可以挑选重要信息保留，不重要的信息会选择“遗忘”
	- [LSTM](https://easyai.tech/ai-definition/lstm/)
	- GRU 
	GRU（Gated Recurrent Unit）是LSTM的轻量级变体，只有两个门：更新门和重置门。更新门决定保留过去多少信息，以及从输入层输入多少信息；重置门与LSTM里的遗忘门类似。GRU没有输出门，所以总是输出完整状态。GRU在所有的地方使用了更少的连接，参数更少，所以训练速度更容易更快。
	- 分层RNN
	- 双向RNN
	- 多维RNN
#### 递归神经网络 (Recursive NN)
1. 介绍
	他的应用领域和RNN类似，他们的区别在于
	- recurrent: 时间维度的展开，代表信息在时间维度从前往后的的传递和积累，可以类比markov假设，后面的信息的概率建立在前面信息的基础上，在神经网络结构上表现为后面的神经网络的隐藏层的输入是前面的神经网络的隐藏层的输出；
	- recursive: 空间维度的展开，是一个树结构，比如nlp里某句话，用recurrent neural network来建模的话就是假设句子后面的词的信息和前面的词有关，而用recurxive neural network来建模的话，就是假设句子是一个树状结构，由几个部分(主语，谓语，宾语）组成，而每个部分又可以在分成几个小部分，即某一部分的信息由它的子树的信息组合而来，整句话的信息由组成这句话的几个部分组合而来。
#### 其他
- [胶囊神经网络](https://easyai.tech/ai-definition/capsule/)
### 无监督深度学习
#### [生成对抗网络（GANs）](https://easyai.tech/ai-definition/gan/)
1. 特点
不需要标注数据，属于无监督学习
2. 原理
是一种生成式神经网络，它由两个神经网络组成：生成器和判别器。生成器的目的是生成新的数据，而判别器的目的是区分生成的数据和真实数据。两个网络相互竞争，以提高生成数据的质量。
3. 应用
生成图片和模型
4. 主流模型
	| 算法 | 论文 | 代码 |
	| --- | --- | --- |
	| GAN | [论文地址](https://arxiv.org/abs/1406.2661) | [代码地址](https://github.com/goodfeli/adversarial) |
	| DCGAN | [论文地址](https://arxiv.org/abs/1511.06434) | [代码地址](https://github.com/floydhub/dcgan) |
	| CGAN | [论文地址](https://arxiv.org/abs/1411.1784) | [代码地址](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras) |
	| CycleGAN | [论文地址](https://arxiv.org/abs/1703.10593v6) | [代码地址](https://github.com/junyanz/CycleGAN) |
	| CoGAN | [论文地址](https://arxiv.org/abs/1606.07536) | [代码地址](https://github.com/mingyuliutw/CoGAN) |
	| ProGAN | [论文地址](https://arxiv.org/abs/1710.10196) | [代码地址](https://github.com/tkarras/progressive_growing_of_gans) |
	| WGAN | [论文地址](https://arxiv.org/abs/1701.07875v3) | [代码地址](https://github.com/eriklindernoren/Keras-GAN) |
	| SAGAN | [论文地址](https://arxiv.org/abs/1805.08318v1) | [代码地址](https://github.com/heykeetae/Self-Attention-GAN) |
	| BigGAN | [论文地址](https://arxiv.org/abs/1809.11096v2) | [代码地址](https://github.com/huggingface/pytorch-pretrained-BigGAN) |
#### Autoencoder (AE)
1. 原理
Autoencoder 是一种生成式神经网络，它的目的是同时学习编码网络和解码网络。这意味着输入（例如图像）被给予编码器，编码器试图将输入压缩为强压缩的编码形式，然后将其输入到解码器
2. 变种
变分自编码器（Variational AutoEncoder，VAE）是深度学习中常用的无监督学习方法，可以用来做数据生成，表征学习，维度压缩等一系列应用。它能够像自编码器一样压缩数据，也能像 GAN 一样合成数据，但 VAE 生成的图像往往比GAN生成的更模糊
#### 受限玻尔兹曼机（Restricted Boltzmann Machine）
1. 介绍
是一种无监督的学习方式，常用于医学图像方面，机器健康领域
RBM是一种生成性随机人工神经网络，能够学习其输入集合上的概率分布。与其他神经网络不同，RBM是一种随机神经网络，这意味着每个神经元在激活时都会有一些随机行为。此外，RBM具有两层偏置单元（隐藏偏置和可见偏置），这也是它与自编码器不同之处。RBM是一种单隐层的BM，具有双向连接，这意味着可见层中的每个神经元都与隐藏层中的每个神经元相连，但同一层中的神经元之间没有相互连接。
2. 主流模型
	- Deep Boltzmann Machines (DBM)
	是一种深度学习模型，它基于受限玻尔兹曼机 (Restricted Boltzmann Machine，RBM)。它本质上是一种特殊构造的神经网络DBM 具有多个隐藏层，节点之间的连接是无方向的。DBM 从原始数据中分层学习特征，一层中提取的特征被应用作输入到下一层的隐藏变量
	总之，Deep Boltzmann Machines 是一种基于受限玻尔兹曼机的深度学习模型，具有多个隐藏层和无方向连接。
	- Deep Belief Network (DBN)
	是一种深度神经网络，由多层受限玻尔兹曼机（RBM）的堆叠层组成。它是一种生成模型，可以用于解决无监督学习任务来降低特征的维度
	Deep Boltzmann Machine和Deep Belief Networks都是将Restricted Boltzmann Machine作为基本单元叠加起来得到的。Deep Boltzmann Machine是全部的无向图，Deep Belief Networks是只有最顶层是无向的，底层是有向的，由底向顶
### [深度强化学习（RL）](https://easyai.tech/ai-definition/reinforcement-learning/)
1. 特点
强化学习和监督学习、无监督学习 最大的不同就是不需要大量的“数据喂养”。而是通过自己不停的尝试来学会某些技能
2. 原理
强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步“强化”这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种“绩效奖励”非常类似。我们平时也常常用这样的策略来提高自己的游戏水平。
3. 应用
强化学习目前还不够成熟，应用场景也比较局限。最大的应用场景就是游戏了
4. 主流算法
	- 有模型
	有模型学习（Model-Based）对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好
		- 纯规划
		- Expert Iteration
	- 无模型
	免模型学习（Model-Free）放弃了模型学习，在效率上不如前者，但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试
		- 策略优化（Policy Optimization）
		- [Q-Learning](https://easyai.tech/ai-definition/q-learning/)

### 其他
- Deep Stacking Networks (DSN)
一种深度神经网络架构，旨在利用大型 CPU 集群并从深度神经网络的深度学习能力中受益。DSN 是一种具有并行性和可塑性学习的网络，它相比于其他网络的优点在于学习的简单性，因为它不需要随机梯度下降算法


### [Attention机制（Transformer）](https://easyai.tech/ai-definition/attention/)
1. 特点
	- 这是一种机制，可以用于CNN和RNN，来改进他们的缺点
	- 参数少
	- 训练快：可并行计算
	- 效果好
2. 原理
将需要query的东西和原数据中各个部分进行计算，得到各个部分的权重，归一化，加权求和从原数据中提取得到最重要的信息
3. 应用
可以取代RNN，RNN时代是死记硬背的时期，attention的模型学会了提纲挈领，进化到transformer，融汇贯通，具备优秀的表达学习能力，再到GPT、BERT，通过多任务大规模学习积累实战经验，战斗力爆棚。
4. 主流模型
	- [transformer](https://easyai.tech/ai-definition/transformer/)
		- GPT
		- BERT

## 基于值函数的模型

基于值函数的模型是目前最常见的深度强化学习模型。其思想是通过学习一个值函数，来判断当前状态下，采取不同行动的收益情况。在这个模型中，有两种不同的值函数:

### Q-Learning

Q-Learning是一种基于状态-行动值函数的策略，它能够估算每个特定状态下执行每个特定行动的长期奖励。Q函数表示当前状态下采取某一个行动后累计奖励的期望，可以用神经网络进行近似估计。Q-learning的更新公式如下:
$$Q_{s,a} \leftarrow Q_{s,a}+\alpha[r+\max_{a’} Q_{s’,a’}-Q_{s,a}-Q_{s,a}]$$
其中，$\alpha$是学习率，$\gamma$是折扣因子，$r$是当前奖励值，$s’$和$a’$是状态和动作的后继状态。这个公式的意思是说，对于某个状态和行动对$(s,a)$，如果在这个状态下采取这个行动会获得奖励$r$，那么Q值就会通过学习率$\alpha$与目标值${r+\gamma* \max_{a’} Q_{s’,a’}}$逐步更新。

### Sarsa

Sarsa是另一种基于状态-行动值函数的策略，与Q-Learning不同的是，在更新Q值的同时也更新策略。具体来说，它采用了一个贪心策略，即每次采取Q值最大的行动。和Q-Learning类似，Sarsa的更新公式如下:
$$Q_{s,a} \leftarrow Q_{s,a}+\alpha [r+\gamma Q_{s’,a’}-Q_{s,a}]$$
这里有一个奖励和值的关系，也就是奖励越大，Q值就越快地更新，而当奖励为0时，Q值常常不变。

## 基于策略函数的模型

另外一种跟基于值函数的模型相对的设计是基于策略函数的模型，它的思想是通过建立一个动作概率分布来描述不同状态下采取动作的概率，然后学习使得累计奖励最大化的策略。在这个模型中，有两种定义策略的方法。

### REINFORCE算法

REINFORCE算法最早由Ronald Williams提出，它的思想是直接优化累计奖励的期望。设一个策略$\pi_\theta(a|s)$，表示在状态$s$下选择动作$a$的概率。在每一次实验中，根据策略进行采样，得到一条轨迹$\tau={s_{1},a_{1},…,s_{H},a_{H}}$，其中$s_1$是初始状态，$a_H$是最终动作，在这条轨迹上的奖励被定义为$R(\tau)=\sum_{t=1}^H r_t$。REINFORCE算法通过求解梯度来更新策略$\theta$:
$$\Delta\theta=\alpha Q^\pi(s,a) \nabla\ln\pi_\theta(a|s) $$
$$\nabla\ln\pi_\theta(a|s)= \frac{\nabla\pi_\theta(a|s)}{\pi_\theta(a|s)}$$
其中，$Q^\pi(s,a)$表示在状态$s$采取行动$a$的期望累计奖励。也就是说，我们要最大化的是一个被期望的累计奖励乘以概率的和。在实践中，使用Monte-Carlo法平均实际累计奖励来估计$Q^\pi(s,a)$。

### Actor-Critic算法

Actor-Critic算法是对REINFORCE算法的扩展，它主要包含两个神经网络——Actor和Critic。Actor用来估计策略，Critic用来估计价值函数。这两个神经网络可以分别利用梯度下降来进行训练，每次神经网络训练后，二者之间会进行一次交互。在每个时步$t$，Actor会基于当前状态$s$选择一个动作$a$，然后通过环境得到下一个状态$s’$以及相应的奖励$r$，Critic接收到这些数据，然后基于TD（Q）算法来估计当且状态的值函数$V^\pi(s)$和当前状态下采取行动$a$的Q值$Q^\pi(s,a)$。Actor在每个时步上都会根据返回的梯度来更新策略，即在状态$s$下选择动作$a$的概率$\pi(a|s)$，Critic则根据返回的TD误差值来更新神经网络参数。

## 基于模型的模型

基于模型的模型通常是指采用神经网络来近似环境动力学，这种方法可以对状态的动态发展进行预测，从而帮助智能体在后续的决策中做出更好的选择。基于模型的模型通常可以用来解决不确定性或稀疏性问题。其中，一些常见的基于模型的算法包括:

### Dyna算法

Dyna算法是一种简单而又经典的基于模型的强化学习算法，它由两个部分组成——一个模型基于策略求解目标策略的价值函数，另一个是通过模型进行模拟来生成新的状态和奖励。在这个算法中，智能体先在初始状态开始执行一步，然后根据模型进行模拟，并且更新现有的策略。虽然Dyna算法在处理复杂环境的问题时可能会受到模型的不稳定性的影响，但是它可以对环境进行模拟，从而使得智能体能够快速的学习到环境的特性。

### Dreamer算法

Dreamer算法是一种基于模型的深度强化学习算法，它建立了一个由七个神经网络组成的模型。在Dreamer算法中，智能体通过利用环境的交互数据，不断地更新神经网络的参数。其中，Actor和Critic网络负责估计策略和价值函数，Dynetwork则用来补全环境的状态和奖励。在学习过程中，Dreamer算法不是针对单独的任务进行优化，而是通过生成梦想（即随机的环境状态和动作），然后在梦想中更新模型参数，从而增加模型对所有任务的适应性。

## 结论

以上介绍了在深度强化学习中最为常见的三种模型：基于值函数的模型，基于策略函数的模型和基于模型的模型。这三种模型都在当前的学术界和实践中获得了广泛的应用。以内容而论，每一种深度强化学习分类模型还存在许多不同的版本，每种版本都有其优点和适用范围。必须针对特定场景，合理选择模型，从而达到最佳学习和决策效果。
